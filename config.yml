run:
  project: LLM                 # wandb-project
  name:    test-1
  seed:    42
  log_interval: 10

model:
  n_dim:       768
  n_blocks:    16
  n_heads:     8
  max_seq_len: 1024
  vocab_size:  -1  # later defined by tokenizer
  norm_eps:    1.0e-5
  dropout:     0.0 # good for pretraining, 0.1 for finetuning
  bias: False

training:
  train_iters:      25000
  eval_iters:       10
  eval_interval:    250
  warmup_iters:     2000
  batch_size:       2
  grad_clip:        1.0
  grad_accum_steps: 8
  use_checkpoint:   true
  ckpt_path:        ./checkpoint.pt

optim:
  max_lr:       6.0e-4
  min_lr:       6.0e-5 # should be ~max_lr/10
  weight_decay: 1.0e-2
  beta1:        0.9
  beta2:        0.95
  eps:          1.0e-8